# -*- coding: utf-8 -*-

# opsiconfd is part of the desktop management solution opsi http://www.opsi.org
# Copyright (c) 2008-2024 uib GmbH <info@uib.de>
# All rights reserved.
# License: AGPL-3.0

"""
opsiconfd - logging
"""
from __future__ import annotations

import asyncio
import glob
import logging as pylogging
import os
import re
import shutil
import socket
import sys
import threading
import time
from asyncio import Event, get_running_loop
from concurrent.futures import ThreadPoolExecutor
from logging import Formatter, LogRecord, PlaceHolder, StreamHandler
from queue import Empty, Queue
from typing import TYPE_CHECKING, Any, Callable, Dict, TextIO

import colorlog
import msgspec
from aiofiles.threadpool import AsyncTextIOWrapper  # type: ignore[import]
from aiologger.handlers.files import AsyncFileHandler  # type: ignore[import]
from aiologger.handlers.streams import AsyncStreamHandler  # type: ignore[import]
from opsicommon.logging import (
	DATETIME_FORMAT,
	LOG_COLORS,
	OPSI_LEVEL_TO_LEVEL,
	SECRET_REPLACEMENT_STRING,
	ContextSecretFormatter,
	RichConsoleHandler,
	context_filter,
	get_logger,
	handle_log_exception,
	secret_filter,
	set_filter_from_string,
	set_format,
)
from opsicommon.logging.constants import NONE, SECRET
from opsicommon.logging.logging import (
	add_context_filter_to_loggers,
)
from redis import BusyLoadingError as RedisBusyLoadingError
from redis import ConnectionError as RedisConnectionError

from opsiconfd.config import REDIS_LOG_ADAPTER_THREAD_POOL_WORKERS, config, opsi_config
from opsiconfd.redis import (
	async_redis_client,
	redis_client,
	retry_redis_call,
)
from opsiconfd.utils import asyncio_create_task

if TYPE_CHECKING:
	from rich.console import Console

# 1 log record ~= 550 bytes
LOG_STREAM_MAX_RECORDS = 50000

redis_log_handler = None
redis_log_adapter_thread = None

# Set default log level to ERROR early
root_logger = get_logger()
root_logger.setLevel(pylogging.ERROR)
logger = get_logger("opsiconfd.general")


async def event_wait(event: Event, timeout: float) -> bool:
	try:
		await asyncio.wait_for(event.wait(), timeout)
	except asyncio.TimeoutError:
		pass
	return event.is_set()


class AsyncRotatingFileHandler(AsyncFileHandler):
	rollover_check_interval: int = 60
	stream: AsyncTextIOWrapper

	def __init__(
		self,
		filename: str,
		formatter: Formatter,
		active_lifetime: int = 0,
		mode: str = "a",
		encoding: str = "utf-8",
		max_bytes: int = 0,
		keep_rotated: int = 0,
		error_handler: Callable | None = None,
	) -> None:
		super().__init__(filename, mode, encoding)
		self.active_lifetime = active_lifetime
		self._max_bytes = max_bytes
		self._keep_rotated = keep_rotated
		self.formatter = formatter
		self._rollover_lock = asyncio.Lock()
		self._rollover_error: Exception | None = None
		self._error_handler = error_handler
		self._should_stop = Event()
		self._periodically_test_rollover_task = asyncio.create_task(self._periodically_test_rollover())
		self.last_used = time.time()

	async def _close_stream(self) -> None:
		try:
			await self.stream.flush()
			await self.stream.close()
		except Exception:
			pass
		self.stream = None

	async def close(self) -> None:
		"""
		flush / close blocks sometimes, processing as task
		"""
		self._should_stop.set()
		if not self.initialized:
			return
		asyncio_create_task(self._close_stream())
		self._initialization_lock = None

	async def _periodically_test_rollover(self) -> None:
		while True:
			try:
				if await get_running_loop().run_in_executor(None, self.should_rollover):
					async with self._rollover_lock:
						await self.do_rollover()
						self._rollover_error = None
			except Exception as err:
				self._rollover_error = err
				logger.error(err, exc_info=True)
			check_interval = 300 if self._rollover_error else self.rollover_check_interval
			if await event_wait(self._should_stop, check_interval):
				break

	def should_rollover(self, record: LogRecord | None = None) -> bool:
		if not os.path.exists(self.absolute_file_path):
			# This will recreate a deleted log file
			return True
		return os.path.getsize(self.absolute_file_path) >= self._max_bytes

	async def do_rollover(self) -> None:
		loop = get_running_loop()
		if self.stream:
			await self.stream.close()
		if self._keep_rotated > 0:
			for num in range(self._keep_rotated, 0, -1):
				src_file_path = self.absolute_file_path
				if num > 1:
					src_file_path = f"{self.absolute_file_path}.{num-1}"
				if not await loop.run_in_executor(None, os.path.exists, src_file_path):
					continue
				dst_file_path = f"{self.absolute_file_path}.{num}"
				await loop.run_in_executor(None, os.rename, src_file_path, dst_file_path)
				try:
					await loop.run_in_executor(
						None,
						shutil.chown,
						dst_file_path,
						config.run_as_user,
						opsi_config.get("groups", "admingroup"),
					)
				except Exception:
					pass
				try:
					await loop.run_in_executor(None, os.chmod, dst_file_path, 0o644)
				except Exception:
					pass
		for filename in await loop.run_in_executor(None, glob.glob, f"{self.absolute_file_path}.*"):
			if isinstance(filename, str):
				try:
					if int(filename.split(".")[-1]) > self._keep_rotated:
						await loop.run_in_executor(None, os.remove, filename)
				except ValueError:
					await loop.run_in_executor(None, os.remove, filename)

		self.stream = None
		await self._init_writer()
		try:
			await loop.run_in_executor(None, os.chmod, self.absolute_file_path, 0o644)
			await loop.run_in_executor(
				None, shutil.chown, self.absolute_file_path, config.run_as_user, opsi_config.get("groups", "admingroup")
			)
		except Exception as err:
			logger.warning(err)

	async def emit(self, record: LogRecord) -> None:
		async with self._rollover_lock:
			self.last_used = time.time()
			return await super().emit(record)

	async def handle_error(self, record: LogRecord, exception: Exception) -> None:
		if self._error_handler:
			await self._error_handler(self, record, exception)


class AsyncRedisLogAdapter:
	def __init__(self, running_event: threading.Event | None = None, stderr_file: TextIO | None = None) -> None:
		self._stderr_file = stderr_file
		if not self._stderr_file:
			self._stderr_file = sys.stderr
		self._running_event = running_event
		self._read_config()
		self._loop = get_running_loop()
		self._redis_log_stream = f"{config.redis_key('log')}:{config.node_name}"
		self._file_logs: Dict[str, AsyncFileHandler] = {}
		self._file_log_active_lifetime = 30
		self._file_log_lock = threading.Lock()
		self._stderr_handler = None
		self._should_stop = Event()
		self._reader_stopped = asyncio.Event()
		self._set_log_format_stderr()

		if self._log_level_file != NONE:
			if self._log_file_template:
				self.get_file_handler()

		asyncio_create_task(self._start(), self._loop)

	async def stop(self) -> None:
		self._should_stop.set()
		for file_log in self._file_logs.values():
			await file_log.close()
		await event_wait(self._reader_stopped, 5.0)

	def reload(self) -> None:
		self._read_config()
		self._set_log_format_stderr()

		for file_handler in self._file_logs.values():
			file_handler.formatter = ContextSecretFormatter(
				Formatter(self._log_format_no_color(self._log_format_file), datefmt=DATETIME_FORMAT)
			)
			file_handler.max_bytes = self._max_log_file_size
			file_handler.keep_rotated = self._keep_rotated_log_files

	def _read_config(self) -> None:
		self._log_file_template = config.log_file
		self._max_log_file_size = round(config.max_log_size * 1000 * 1000)
		self._keep_rotated_log_files = config.keep_rotated_logs
		self._symlink_client_log_files = config.symlink_logs
		self._log_level_stderr = OPSI_LEVEL_TO_LEVEL[config.log_level_stderr]
		self._log_level_file = OPSI_LEVEL_TO_LEVEL[config.log_level_file]
		self._log_format_stderr = config.log_format_stderr
		self._log_format_file = config.log_format_file

	def _set_log_format_stderr(self) -> None:
		if self._log_level_stderr == NONE:
			self._stderr_handler = None
			return
		console_formatter: Formatter
		if sys.stderr.isatty():
			# colorize
			console_formatter = colorlog.ColoredFormatter(self._log_format_stderr, log_colors=LOG_COLORS, datefmt=DATETIME_FORMAT)
		else:
			console_formatter = Formatter(self._log_format_no_color(self._log_format_stderr), datefmt=DATETIME_FORMAT)
		if not self._stderr_handler:
			self._stderr_handler = AsyncStreamHandler(stream=self._stderr_file)
		self._stderr_handler.formatter = ContextSecretFormatter(console_formatter)
		self._stderr_handler.formatter.secret_filter_enabled = False  # Secrets are filtered before records are written to redis
		self._stderr_handler.add_filter(context_filter.filter)

	def _log_format_no_color(self, log_format: str) -> str:
		return log_format.replace("%(log_color)s", "").replace("%(reset)s", "")

	async def _create_client_log_file_symlink(self, ip_address: str) -> None:
		try:
			fqdn = await self._loop.run_in_executor(None, socket.getfqdn, ip_address)
			if fqdn != ip_address:
				src = self._log_file_template.replace("%m", ip_address)
				src = os.path.basename(src)
				dst = self._log_file_template.replace("%m", fqdn)
				if not os.path.exists(dst):
					await self._loop.run_in_executor(None, os.symlink, src, dst)
		except Exception as exc:
			logger.error(exc, exc_info=True)

	async def handle_file_handler_error(self, file_handler: AsyncFileHandler, record: LogRecord, exception: Exception) -> None:
		if not isinstance(exception, RuntimeError):
			handle_log_exception(exception, record, stderr=True, temp_file=True)
		if file_handler.absolute_file_path in self._file_logs:
			await self._file_logs[file_handler.absolute_file_path].close()
			del self._file_logs[file_handler.absolute_file_path]

	def get_file_handler(self, client: str | None = None) -> AsyncRotatingFileHandler | None:
		filename = None
		if not self._log_file_template:
			return None
		try:
			name = client or "opsiconfd"
			filename = os.path.abspath(self._log_file_template.replace("%m", name))
			with self._file_log_lock:
				if not self._file_logs.get(filename):
					logger.info("Creating new file log '%s'", filename)
					log_dir = os.path.dirname(filename)
					if not os.path.isdir(log_dir):
						logger.info("Creating log dir '%s'", log_dir)
						os.makedirs(log_dir)
					# Do not close main opsiconfd log file
					active_lifetime = 0 if name == "opsiconfd" else self._file_log_active_lifetime
					formatter = ContextSecretFormatter(Formatter(self._log_format_no_color(self._log_format_file), datefmt=DATETIME_FORMAT))
					formatter.secret_filter_enabled = False  # Secrets are filtered before records are written to redis
					self._file_logs[filename] = AsyncRotatingFileHandler(
						filename=filename,
						formatter=formatter,
						active_lifetime=active_lifetime,
						mode="a",
						encoding="utf-8",
						max_bytes=self._max_log_file_size,
						keep_rotated=self._keep_rotated_log_files,
						error_handler=self.handle_file_handler_error,
					)
					if client and self._symlink_client_log_files:
						asyncio_create_task(self._create_client_log_file_symlink(client), self._loop)
				self._file_logs[filename].add_filter(context_filter.filter)
				return self._file_logs[filename]
		except Exception as exc:
			if filename in self._file_logs:
				del self._file_logs[filename]
			handle_log_exception(exc, stderr=True, temp_file=True)
		return None

	async def _watch_log_files(self) -> None:
		if not self._log_file_template:
			return
		while True:
			try:
				for filename in list(self._file_logs):
					if not self._file_logs[filename] or self._file_logs[filename].active_lifetime == 0:
						continue
					time_diff = time.time() - self._file_logs[filename].last_used
					if time_diff > self._file_logs[filename].active_lifetime:
						with self._file_log_lock:
							logger.info(
								"Closing inactive file log '%s', file logs remaining active: %d",
								filename,
								len(self._file_logs) - 1,
							)
							await self._file_logs[filename].close()
							del self._file_logs[filename]
			except Exception as err:
				logger.error(err, exc_info=True)

			if await event_wait(self._should_stop, 60.0):
				return

	async def _start(self) -> None:
		try:
			redis = await async_redis_client(timeout=30, test_connection=True)
			await redis.xtrim(name=self._redis_log_stream, maxlen=LOG_STREAM_MAX_RECORDS, approximate=True)
			asyncio_create_task(self._reader(), self._loop)
			asyncio_create_task(self._watch_log_files(), self._loop)

		except Exception as err:
			handle_log_exception(err, stderr=True, temp_file=True)
			if self._running_event:
				self._running_event.set()

	async def _reader(self) -> None:
		if self._running_event:
			self._running_event.set()

		msgpack_decoder = msgspec.msgpack.Decoder()
		last_id = "$"
		redis = await async_redis_client()
		while True:
			try:
				# It is also possible to specify multiple streams
				data = await redis.xread(streams={self._redis_log_stream: last_id}, block=1000)
				if self._should_stop.is_set():
					break
				if not data:
					continue
				for stream in data:
					for entry in stream[1]:
						last_id = entry[0]
						client = entry[1].get(b"client_address", b"").decode("utf-8")
						record_dict = msgpack_decoder.decode(entry[1][b"record"])
						record_dict.update({"scope": None, "exc_info": None, "args": None})
						record = pylogging.makeLogRecord(record_dict)

						if record.levelno >= self._log_level_file:
							file_handler = self.get_file_handler(client)
							if file_handler:
								await file_handler.handle(record)

						if self._stderr_handler and record.levelno >= self._log_level_stderr:
							await self._stderr_handler.handle(record)

			except (KeyboardInterrupt, SystemExit):
				raise
			except EOFError:
				break
			except (RedisConnectionError, RedisBusyLoadingError):
				pass
			except Exception as err:
				handle_log_exception(err, stderr=True, temp_file=True)

		self._reader_stopped.set()


class RedisLogHandler(pylogging.Handler, threading.Thread):
	"""
	Will collect log messages in pipeline and send collected
	log messages at once to redis in regular intervals.
	"""

	def __init__(self, max_msg_len: int = 0, max_delay: float = 0.1) -> None:
		pylogging.Handler.__init__(self)
		threading.Thread.__init__(self)
		self._name = "RedisLogHandlerThread"
		self._max_msg_len = max_msg_len
		self._max_delay = max_delay
		self._redis_log_stream = f"{config.redis_key('log')}:{config.node_name}"
		self._queue: Queue = Queue()
		self._should_stop = threading.Event()
		self._stopped = threading.Event()
		self._msgpack_encoder = msgspec.msgpack.Encoder()
		self.start()

	@property
	def name(self) -> str:  # type: ignore[override]
		return self._name

	def run(self) -> None:
		try:
			# Trim redis log stream to max size
			redis_client().xtrim(f"{self._redis_log_stream}", maxlen=LOG_STREAM_MAX_RECORDS, approximate=True)
			self._process_queue()
		except Exception as err:
			logger.error(err, exc_info=True)

	@retry_redis_call
	def _process_queue(self) -> None:
		while True:
			self._should_stop.wait(self._max_delay)
			if self._queue.qsize() > 0:
				pipeline = redis_client().pipeline()
				try:
					while True:
						pipeline.xadd(
							self._redis_log_stream,
							self._queue.get_nowait(),
							maxlen=LOG_STREAM_MAX_RECORDS,
							approximate=True,
						)
				except Empty:
					pass
				pipeline.execute()
			if self._should_stop.is_set():
				break

	def stop(self) -> None:
		self._should_stop.set()
		self._stopped.wait(3.0)

	def log_record_to_dict(self, record: LogRecord) -> Dict[str, Any]:
		try:
			msg = record.getMessage()
		except TypeError:
			msg = record.msg
		if self.level != SECRET:
			for secret in secret_filter.secrets:
				msg = msg.replace(secret, SECRET_REPLACEMENT_STRING)
		if self._max_msg_len and len(msg) > self._max_msg_len:
			msg = msg[: self._max_msg_len - 1] + "…"

		if hasattr(record, "exc_info") and record.exc_info:
			# by calling format the formatted exception information is cached in attribute exc_text
			self.format(record)
			record.exc_info = None

		rec_dict = record.__dict__.copy()
		rec_dict["msg"] = msg
		for attr in ("scope", "exc_info", "args", "contextstring", "websocket"):
			if attr in rec_dict:
				del rec_dict[attr]
		return rec_dict

	def emit(self, record: LogRecord) -> None:
		try:
			entry = {}
			context = getattr(record, "context", None)
			if context:
				entry = dict(context)
			entry["record"] = self._msgpack_encoder.encode(self.log_record_to_dict(record))
			self._queue.put(entry)
		except (KeyboardInterrupt, SystemExit):
			raise
		except Exception as exc:
			handle_log_exception(exc, record, stderr=True, temp_file=True)


def enable_slow_callback_logging(slow_callback_duration: float | None = None) -> None:
	_run_orig = asyncio.events.Handle._run
	if slow_callback_duration is None:
		slow_callback_duration = get_running_loop().slow_callback_duration

	def _run(self: asyncio.events.Handle) -> int | None:
		start = time.perf_counter()
		retval = _run_orig(self)
		time_diff = time.perf_counter() - start
		if slow_callback_duration and time_diff >= slow_callback_duration:
			logger.warning(
				"Slow asyncio callback: %s took %.3f seconds",
				asyncio.base_events._format_handle(self),  # type: ignore[attr-defined]
				time_diff,
			)
		return retval

	asyncio.events.Handle._run = _run  # type: ignore[assignment]


def init_logging(log_mode: str = "redis", is_worker: bool = False, console: Console | None = None) -> None:
	redis_error = None
	try:
		if log_mode not in ("redis", "local", "rich"):
			raise ValueError(f"Invalid log mode '{log_mode}'")

		log_level = max(config.log_level, config.log_level_stderr, config.log_level_file)
		if log_mode in ("local", "rich"):
			log_level = config.log_level_stderr
		log_level = OPSI_LEVEL_TO_LEVEL[log_level]
		log_handler: pylogging.Handler

		if log_mode == "redis":
			try:
				global redis_log_handler
				if not redis_log_handler:
					redis_log_handler = RedisLogHandler(max_msg_len=int(config.log_max_msg_len))
				log_handler = redis_log_handler
			except Exception as err:
				redis_error = err
				log_mode = "local"

		if log_mode == "local":
			log_handler = StreamHandler(stream=sys.stderr)
		elif log_mode == "rich":
			if not console:
				# Import is slow, only import if needed
				from rich.console import Console

				console = Console()
			log_handler = RichConsoleHandler(console=console)

		log_handler.setLevel(log_level)
		root_logger.handlers = [log_handler]
		root_logger.setLevel(log_level)
		set_format(stderr_format=config.log_format_stderr, file_format=config.log_format_file)

		if config.log_filter:
			set_filter_from_string(config.log_filter)

		for logger_name in ("asyncio", "uvicorn.error", "uvicorn.access", "wsgidav"):
			logger_ = pylogging.getLogger(logger_name)
			logger_.handlers = [log_handler]
			logger_.propagate = False

		if config.log_levels:
			loggers = {
				getattr(logger_, "name"): logger_
				for logger_ in list(pylogging.Logger.manager.loggerDict.values())
				if hasattr(logger_, "name")
			}
			logger_level_configs = {}
			for entry in [entry.strip() for entry in config.log_levels.split(",") if entry.strip()]:
				logger_re, level = entry.rsplit(":", 1)
				logger_level_configs[logger_re.strip()] = int(level.strip())

			# Sort by regex length so the closest match will be applied at last
			for logger_re in sorted(logger_level_configs, key=len):
				level = logger_level_configs[logger_re]
				logger_re = re.compile(logger_re)
				for logger_name, logger_obj in loggers.items():
					if isinstance(logger_obj, PlaceHolder):
						continue
					if logger_re.match(logger_name):
						if level < 10:
							level = OPSI_LEVEL_TO_LEVEL[level]
						logger_obj.setLevel(level)

		add_context_filter_to_loggers()

		if config.log_slow_async_callbacks > 0:
			enable_slow_callback_logging(config.log_slow_async_callbacks)

		if not is_worker:
			if log_mode == "redis" and (config.log_level_stderr != NONE or config.log_level_file != NONE):
				start_redis_log_adapter_thread()
			else:
				stop_redis_log_adapter_thread()

		if redis_error:
			logger.critical("Failed to initalize redis logging: %s", redis_error, exc_info=True)

	except Exception as exc:
		handle_log_exception(exc, stderr=True, temp_file=True)


def shutdown_logging() -> None:
	if redis_log_handler:
		redis_log_handler.stop()
	stop_redis_log_adapter_thread()


class RedisLogAdapterThread(threading.Thread):
	def __init__(self, running_event: threading.Event | None = None) -> None:
		threading.Thread.__init__(self)
		self.name = "RedisLogAdapterThread"
		self._running_event = running_event
		self._redis_log_adapter: AsyncRedisLogAdapter | None = None
		self._loop: asyncio.AbstractEventLoop | None = None

	def stop(self) -> None:
		if self._redis_log_adapter and self._loop:
			asyncio_create_task(self._redis_log_adapter.stop(), self._loop)

	def reload(self) -> None:
		if self._redis_log_adapter:
			self._redis_log_adapter.reload()

	def run(self) -> None:
		try:
			self._loop = asyncio.new_event_loop()
			self._loop.set_default_executor(
				ThreadPoolExecutor(
					max_workers=REDIS_LOG_ADAPTER_THREAD_POOL_WORKERS, thread_name_prefix="RedisLogAdapterThread-ThreadPoolExecutor"
				)
			)
			self._loop.set_debug("asyncio" in config.debug_options)
			asyncio.set_event_loop(self._loop)

			def handle_asyncio_exception(loop: asyncio.AbstractEventLoop, context: dict) -> None:
				if loop.is_running():
					msg = context.get("exception", context["message"])
					print(f"Unhandled exception in RedisLogAdapterThread asyncio loop: {msg}", file=sys.stderr)

			self._loop.set_exception_handler(handle_asyncio_exception)
			asyncio_create_task(self.create_redis_log_adapter(), self._loop)
			self._loop.run_forever()
		except Exception as exc:
			logger.error(exc, exc_info=True)

	async def create_redis_log_adapter(self) -> None:
		self._redis_log_adapter = AsyncRedisLogAdapter(running_event=self._running_event)


def start_redis_log_adapter_thread() -> None:
	global redis_log_adapter_thread
	if redis_log_adapter_thread:
		redis_log_adapter_thread.reload()
		return
	running_event = threading.Event()
	redis_log_adapter_thread = RedisLogAdapterThread(running_event)
	redis_log_adapter_thread.daemon = True
	redis_log_adapter_thread.start()
	running_event.wait()


def stop_redis_log_adapter_thread() -> None:
	if not redis_log_adapter_thread:
		return
	redis_log_adapter_thread.stop()
